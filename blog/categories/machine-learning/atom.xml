<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: machine-learning | 研究研究]]></title>
  <link href="http://www.yanjiuyanjiu.com/blog/categories/machine-learning/atom.xml" rel="self"/>
  <link href="http://www.yanjiuyanjiu.com/"/>
  <updated>2013-04-03T23:50:48+08:00</updated>
  <id>http://www.yanjiuyanjiu.com/</id>
  <author>
    <name><![CDATA[soulmachine]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[机器学习的一些通俗易懂的tutorial]]></title>
    <link href="http://www.yanjiuyanjiu.com/blog/20130327/"/>
    <updated>2013-03-27T21:50:00+08:00</updated>
    <id>http://www.yanjiuyanjiu.com/blog/some-classical-machine-learning-tutorials</id>
    <content type="html"><![CDATA[<h2>距离和相似度度量</h2>

<p><a href="http://webdataanalysis.net/reference-and-source/distance-and-similarity/">距离和相似度度量 » webdataanalysis.net</a></p>

<p><a href="http://www.zhihu.com/question/19640394">欧氏距离和余弦相似度的区别是什么？ – 知乎</a></p>

<h2>KNN(K Nearest Neighbor)</h2>

<p><a href="http://coolshell.cn/articles/8052.html">K Nearest Neighbor 算法 _ 酷壳 – CoolShell</a></p>

<p><a href="http://en.wikipedia.org/wiki/KNN">K-nearest neighbors algorithm – Wikipedia</a></p>

<h2>K-Means</h2>

<p><a href="http://coolshell.cn/articles/7779.html">K-Means 算法 _ 酷壳 – CoolShell</a></p>

<p><a href="http://en.wikipedia.org/wiki/K-means">k-means clustering – Wikipedia</a></p>

<p><a href="http://kylen314.blog.com/2012/09/10/k-means/">K-Means++ _ 愈宅屋</a></p>

<p><a href="http://www.cnblogs.com/leoo2sk/archive/2010/09/20/k-means.html">算法杂货铺——k均值聚类(K-means) – T2噬菌体 – 博客园</a></p>

<p><a href="http://blog.pluskid.org/?p=17">漫谈 Clustering (1)_ k-means « Free Mind</a></p>

<p><a href="http://www.codeproject.com/Articles/439890/Text-Documents-Clustering-using-K-Means-Algorithm">Text Documents Clustering using K-Means Algorithm – CodeProject</a></p>

<!-- more -->


<h2>PCA(Principal Components Analysis)</h2>

<p><a href="http://www.ce.yildiz.edu.tr/personal/songul/file/1097/principal_components.pdf">2002. Lindsay I Smith. A tutorial on Principal Components Analysis</a></p>

<h2>EM(Expectation Maximization)</h2>

<p><a href="http://www.seanborman.com/publications/EM_algorithm.pdf">2009. Sean Borman. The Expectation Maximization Algorithm A short tutorial</a></p>

<h2>SVM(Support Vector Machines)</h2>

<p><a href="http://cs229.stanford.edu/notes/cs229-notes3.pdf">Andrew Ng. CS229 Lecture notes Support Vector Machines</a></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[数值计算库与科学计算库]]></title>
    <link href="http://www.yanjiuyanjiu.com/blog/20130226/"/>
    <updated>2013-02-26T23:15:00+08:00</updated>
    <id>http://www.yanjiuyanjiu.com/blog/numerical-or-scientific-computation-library</id>
    <content type="html"><![CDATA[<h2>BLAS 接口</h2>

<p><a href="http://www.netlib.org/blas/">BLAS</a>, <a href="http://www.netlib.org/lapack/">LAPACK</a>, <a href="http://math-atlas.sourceforge.net/">ATLAS</a> 这些数值计算库的名字很类似，他们之间有什么关系呢？BLAS是一组线性代数运算接口，目前是事实上的标准，很多数值计算/科学计算都实现了这套接口。</p>

<p>BLAS定义了那些函数呢？可以查看<a href="http://www.netlib.org/blas/">官方文档</a>。</p>

<p>LAPACK是BLAS的第一个实现，是最老牌的数值计算库，用FORTRAN 77语言写的。LAPACK实现了BLAS接口，并扩充了一些功能。很多数值计算库/科学计算库底层调用了LAPACK。</p>

<p>很多硬件厂商都实现BLAS接口，例如<a href="http://software.intel.com/en-us/intel-mkl">Intel MKL</a>(Math Kernel Library), <a href="http://developer.amd.com/tools/cpu-development/amd-core-math-library-acml/">AMCL</a>(AMD Math Core Library)等。很多开源库也支持，例如ATLAS。</p>

<p>还有非常多的库实现了BLAS接口，见<a href="http://en.wikipedia.org/wiki/Basic_Linear_Algebra_Subprograms">Wikipedia BLAS</a> 的Implementations小节。</p>

<p>下面介绍一些各种语言常用的数值计算/科学计算库。</p>

<!-- more -->


<h2>C/C++</h2>

<p>首先是Intel 的MKL 和 AMD 的AMCL，性能一流，不过是商业软件，价格昂贵。</p>

<p><a href="http://www.gnu.org/software/gsl/">GSL - GNU Scientific Library</a>，GNU实现的库，质量很高，不过是用纯C写的，用起来比较繁琐。</p>

<p><a href="http://arma.sourceforge.net/">Armadillo</a>，最新版 2013-02-20 Version 3.6.3</p>

<p><a href="http://itpp.sourceforge.net/">IT++</a>，最后版本是4.2,2010-09-21。</p>

<h2>Java</h2>

<p>这个页面<a href="http://math.nist.gov/javanumerics/">JavaNumerics page</a>专门收集了关于Java数值计算的库。</p>

<p><a href="https://code.google.com/p/java-matrix-benchmark/">java-matrix-benchmark</a>这个开源项目，比较了各类Java线性代数库的性能。</p>

<p>下面介绍一些影响力较大的java数值计算/科学计算库。</p>

<p><a href="http://commons.apache.org/proper/commons-math/">Commons Math</a>, 最新版本是3.1.1,2013年1月9号发布。这个库提供一些基本的数学运算，没有high-level的东西，例如矩阵，向量等，用起来会比较繁琐。</p>

<p><a href="http://math.nist.gov/javanumerics/jama/">JAMA</a>, 最新版是Version 1.0.3 (November 9, 2012)。</p>

<p><a href="http://acs.lbl.gov/software/colt/">Colt</a>，已经不更新了，最后版本是1.2.0，2004年9月发布的。</p>

<p>Apache Mahout使用了Colt作为high performance collections，见官方<a href="https://cwiki.apache.org/MAHOUT/mahout-collections.html">这个页面</a>，说“The implementation of Mahout Collections is derived from Cern Colt”，以及quora 这个帖子<a href="http://www.quora.com/Distributed-Algorithms/What-are-the-best-resources-for-distributed-numerical-analysis-matrix-algorithms">What are the best resources for distributed numerical analysis/matrix algorithms</a>。</p>

<h2>Python</h2>

<p>目前最有影响力的莫过于<a href="http://www.numpy.org/">NumPy</a>和<a href="http://www.scipy.org/">SciPy</a>。Amazon.com上可以搜到专门讲它们的书。</p>

<p>SciPy依赖NumPy，主要是在数值计算方面调用了NumPy。</p>

<h2>Ruby</h2>

<p><a href="http://sciruby.com/">SciRuby</a>, 是SciPy和NumPy的克隆，目前还在开发中。</p>

<h2>R</h2>

<p>R刚开始时是统计学家开发的语言，专门用于数理统计，现在功能不断增强，内置了很多数值计算和科学计算的功能。R在数据分析领域比较火。</p>

<h2>Scala</h2>

<p>目前用google搜索 “scala numerical computing”，能找得到的就是<a href="http://code.google.com/p/scalalab/">ScalaLab</a>了。</p>

<h2>Matlab</h2>

<p>最后，别忘了Matlab是支持多语言调用的。</p>

<p>可以用Matlab生成DLL，给C/C++语言调用。其实，凡是能调用DLL的语言，都可以使用这个DLL，例如Python, Ruby等。</p>

<p>可以用<a href="http://www.mathworks.cn/products/javabuilder/">Matlab JavaBuilder</a>将m文件转换为jar文件，然后在java代码中就可以调用了。</p>

<h2>如何选择</h2>

<p>本文的重点在于选择一个高性能，同时又比较易用的库，即被让我们调用，用来写程序的库，不是一个集成环境或REPL环境。因此R和Matlab不在讨论范围内。R和Matlab用来做原型或前期Data Exploration比较适合。</p>

<p>选择一个工具（语言，框架，库等），要看其是否成熟。我个人的一些判断指标，主要有</p>

<ol>
<li>有没有大厂商的支持（作为vendor之类的）；</li>
<li>amazon.com上能否搜到书。</li>
</ol>


<p>从厂商的支持来看，几个主要的大厂商如 Intel，AMD和Apple都开发了自己的数学库。Python则有很成熟的NumPy，在Amazon上能搜到书，例如“SciPy and NumPy”， “NumPy Cookbook”。 因此，目前来看，C++和Python是比较成熟的方案。</p>

<h2>参考资料</h2>

<p><a href="http://en.wikipedia.org/wiki/Basic_Linear_Algebra_Subprograms">Wikipedia BLAS</a><br/>
<a href="http://en.wikipedia.org/wiki/LAPACK">Wikipedia LAPACK</a><br/>
<a href="http://blog.henix.info/blog/blas-lapack-do-matrix-operation.html">用 BLAS/LAPACK 编写矩阵运算程序</a><br/>
<a href="https://wikis.utexas.edu/display/~cdupree/BLAS,+LAPACK,+ATLAS">BLAS, LAPACK, ATLAS</a><br/>
<a href="http://hi.baidu.com/luckykele2012/item/6a3b25423018c40d6dc2f090">BLAS 和 LAPACK ，以及其他常用数值计算库</a><br/>
<a href="http://fdatamining.blogspot.com/2011/10/any-numerical-computing-environment-on.html">Any numerical computing environment on Java platform</a><br/>
<a href="http://www.myoutsourcedbrain.com/2009/04/c-libraries-for-numerical-processing.html">C++ Libraries for Scientific Computing</a><br/>
<a href="http://stackoverflow.com/questions/3121139/scientific-library-options-for-c-or-c">Scientific Library Options for C or C++</a><br/>
<a href="http://programmers.stackexchange.com/questions/138643/why-is-python-used-for-high-performance-scientific-computing-but-ruby-isnt">Why is Python used for high-performance/scientific computing (but Ruby isn't)?</a></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[KNN与K-Means的区别]]></title>
    <link href="http://www.yanjiuyanjiu.com/blog/20130225/"/>
    <updated>2013-02-25T23:41:00+08:00</updated>
    <id>http://www.yanjiuyanjiu.com/blog/differences-between-knn-and-kmeans</id>
    <content type="html"><![CDATA[<h2>KNN(K-Nearest Neighbor)介绍</h2>

<p>Wikipedia上的<a href="http://en.wikipedia.org/wiki/K-nearest_neighbor_algorithm">KNN词条</a>中有一个比较经典的图如下：</p>

<p><img src="http://yanjiuyanjiu-wordpress.stor.sinaapp.com/uploads/2013/02/022513_0955_KNNKMeans1.png"></p>

<p>KNN的算法过程是是这样的：</p>

<p>从上图中我们可以看到，图中的数据集是良好的数据，即都打好了label，一类是蓝色的正方形，一类是红色的三角形，那个绿色的圆形是我们待分类的数据。</p>

<p>如果K=3，那么离绿色点最近的有2个红色三角形和1个蓝色的正方形，这3个点投票，于是绿色的这个待分类点属于红色的三角形。</p>

<p>如果K=5，那么离绿色点最近的有2个红色三角形和3个蓝色的正方形，这5个点投票，于是绿色的这个待分类点属于蓝色的正方形。（参考 <a href="http://coolshell.cn/articles/8052.html">酷壳的 K Nearest Neighbor 算法</a>）</p>

<p>我们可以看到，KNN本质是基于一种数据统计的方法！其实很多机器学习算法也是基于数据统计的。</p>

<!-- more -->


<p>KNN是一种memory-based learning，也叫instance-based learning，属于lazy learning。即它没有明显的前期训练过程，而是程序开始运行时，把数据集加载到内存后，不需要进行训练，就可以开始分类了。</p>

<p>具体是每次来一个未知的样本点，就在附近找K个最近的点进行投票。</p>

<p>再举一个例子，Locally weighted regression (LWR)也是一种 memory-based 方法，如下图所示的数据集。</p>

<p><img src="http://yanjiuyanjiu-wordpress.stor.sinaapp.com/uploads/2013/02/022513_0955_KNNKMeans2.gif"></p>

<p>用任何一条直线来模拟这个数据集都是不行的，因为这个数据集看起来不像是一条直线。但是每个局部范围内的数据点，可以认为在一条直线上。每次来了一个位置样本x，我们在X轴上以该数据样本为中心，左右各找几个点，把这几个样本点进行线性回归，算出一条局部的直线，然后把位置样本x代入这条直线，就算出了对应的y，完成了一次线性回归。</p>

<p>也就是每次来一个数据点，都要训练一条局部直线，也即训练一次，就用一次。</p>

<p>LWR和KNN是不是很像？都是为位置数据量身定制，在局部进行训练。</p>

<h2>K-Means介绍</h2>

<p><img src="http://yanjiuyanjiu-wordpress.stor.sinaapp.com/uploads/2013/02/022513_0955_KNNKMeans3.jpg"></p>

<p>如图所示，数据样本用圆点表示，每个簇的中心点用叉叉表示。(a)刚开始时是原始数据，杂乱无章，没有label，看起来都一样，都是绿色的。(b)假设数据集可以分为两类，令K=2，随机在坐标上选两个点，作为两个类的中心点。(c-f)演示了聚类的两种迭代。先划分，把每个数据样本划分到最近的中心点那一簇；划分完后，更新每个簇的中心，即把该簇的所有数据点的坐标加起来去平均值。这样不断进行”划分—更新—划分—更新”，直到每个簇的中心不在移动为止。(图文来自Andrew ng的机器学习公开课)。</p>

<p>推荐关于K-Means的两篇博文，<a href="http://coolshell.cn/articles/7779.html">K-Means 算法 _ 酷壳</a>，<a href="http://blog.pluskid.org/?p=17">漫谈 Clustering (1)_ k-means pluskid</a>。</p>

<h2>KNN和K-Means的区别</h2>

<table style="border-collapse: collapse;" border="0">
<colgroup>
<col style="width: 277px;">
<col style="width: 277px;"></colgroup>
<tbody valign="top">
<tr>
<td style="padding-left: 7px; padding-right: 7px; border: solid 0.5pt;">
<p style="text-align: center;"><span style="font-size: 10pt;"><strong>KNN</strong></span></p>
</td>
<td style="padding-left: 7px; padding-right: 7px; border-top: solid 0.5pt; border-left: none; border-bottom: solid 0.5pt; border-right: solid 0.5pt;">
<p style="text-align: center;"><span style="font-size: 10pt;"><strong>K-Means</strong></span></p>
</td>
</tr>
<tr style="height: 85px;">
<td style="padding-left: 7px; padding-right: 7px; border-top: none; border-left: solid 0.5pt; border-bottom: solid 0.5pt; border-right: solid 0.5pt;"><span style="font-size: 10pt;">1.KNN是分类算法<br>
</span><p></p>
<p><span style="font-size: 10pt;">2.监督学习<br>
</span></p>
<p style="text-align: justify;"><span style="font-size: 10pt;">3.喂给它的数据集是带label的数据，已经是完全正确的数据</span></p>
</td>
<td style="padding-left: 7px; padding-right: 7px; border-top: none; border-left: none; border-bottom: solid 0.5pt; border-right: solid 0.5pt;"><span style="font-size: 10pt;">1.K-Means是聚类算法<br>
</span><p></p>
<p><span style="font-size: 10pt;">2.非监督学习<br>
</span></p>
<p style="text-align: justify;"><span style="font-size: 10pt;">3.喂给它的数据集是无label的数据，是杂乱无章的，经过聚类后才变得有点顺序，先无序，后有序</span></p>
</td>
</tr>
<tr>
<td style="padding-left: 7px; padding-right: 7px; border-top: none; border-left: solid 0.5pt; border-bottom: solid 0.5pt; border-right: solid 0.5pt;"><span style="font-size: 10pt;">没有明显的前期训练过程，属于memory-based learning</span></td>
<td style="padding-left: 7px; padding-right: 7px; border-top: none; border-left: none; border-bottom: solid 0.5pt; border-right: solid 0.5pt;"><span style="font-size: 10pt;">有明显的前期训练过程</span></td>
</tr>
<tr>
<td style="padding-left: 7px; padding-right: 7px; border-top: none; border-left: solid 0.5pt; border-bottom: solid 0.5pt; border-right: solid 0.5pt;"><span style="font-size: 10pt;">K的含义：来了一个样本x，要给它分类，即求出它的y，就从数据集中，在x附近找离它最近的K个数据点，这K个数据点，类别c占的个数最多，就把x的label设为c</span></td>
<td style="padding-left: 7px; padding-right: 7px; border-top: none; border-left: none; border-bottom: solid 0.5pt; border-right: solid 0.5pt;"><span style="font-size: 10pt;">K的含义：K是人工固定好的数字，假设数据集合可以分为K个簇，由于是依靠人工定好，需要一点先验知识</span></td>
</tr>
<tr>
<td style="padding-left: 7px; padding-right: 7px; border-top: none; border-left: solid 0.5pt; border-bottom: solid 0.5pt; border-right: solid 0.5pt;"></td>
<td style="padding-left: 7px; padding-right: 7px; border-top: none; border-left: none; border-bottom: solid 0.5pt; border-right: solid 0.5pt;"></td>
</tr>
<tr>
<td style="padding-left: 7px; padding-right: 7px; border-top: none; border-left: solid 0.5pt; border-bottom: solid 0.5pt; border-right: solid 0.5pt;" colspan="2"><span style="font-size: 10pt;">相似点：都包含这样的过程，给定一个点，在数据集中找离它最近的点。即二者都用到了NN(Nears Neighbor)算法，一般用KD树来实现NN。</span></td>
</tr>
</tbody>
</table>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[基于朴素贝叶斯的文本分类算法]]></title>
    <link href="http://www.yanjiuyanjiu.com/blog/20100528/"/>
    <updated>2010-05-28T17:15:00+08:00</updated>
    <id>http://www.yanjiuyanjiu.com/blog/text-classification-algorithm-based-on-naive-bayes</id>
    <content type="html"><![CDATA[<p>作者: 灵魂机器<br/>
新浪博客：<a href="www.weibo.com/soulmachine">www.weibo.com/soulmachine</a><br/>
作者博客：<a href="www.yanjiuyanjiu.com">www.yanjiuyanjiu.com</a></p>

<p><strong>摘要</strong>：常用的文本分类方法有支持向量机、K-近邻算法和朴素贝叶斯。其中朴素贝叶斯具有容易实现，运行速度快的特点，被广泛使用。本文详细介绍了朴素贝叶斯的基本原理，讨论了两种常见模型：多项式模型（MM）和伯努利模型（BM），实现了可运行的代码，并进行了一些数据测试。</p>

<p><strong>关键字</strong>：朴素贝叶斯；文本分类</p>

<p><strong>Text Classification Algorithm Based on Naive Bayes</strong>
<strong>Author</strong>: soulmachine
<strong>Email</strong>：soulmachine@gmail.com
<strong>Blog</strong>：<a href="www.yanjiuyanjiu.com">www.yanjiuyanjiu.com</a></p>

<p><strong>Abstract</strong>:Usually there are three methods for text classification: SVM、KNN and Naïve Bayes. Naïve Bayes is easy to implement and fast, so it is widely used. This article introduced the theory of Naïve Bayes and discussed two popular models: multinomial model(MM) and Bernoulli model(BM) in details, implemented runnable code and performed some data tests.</p>

<p><strong>Keywords</strong>: naïve bayes; text classification</p>

<h2>1. 贝叶斯原理</h2>

<h3>1.1 贝叶斯公式</h3>
]]></content>
  </entry>
  
</feed>
